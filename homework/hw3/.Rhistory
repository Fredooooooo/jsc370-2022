knitr::opts_chunk$set(include  = TRUE)
library(stringr)
library(tidytext)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2-vaccine")
knitr::opts_chunk$set(include  = TRUE)
library(stringr)
library(tidytext)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2-vaccine")
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
counts <- stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
counts <- stringr::str_remove_all(counts, "<Count>|</Count>")
counts
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
ids <- ids[1:250]
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = paste(ids, collapse=","),
retmax = 250,
rettype = "abstract"
)
)
# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
abstracts <- stringr::str_extract(pub_char_list, "<Abstract>(\\n|.)+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alpha:]]+>")
abstracts <- str_replace_all(abstracts, "\\s+", " ")
table(is.na(abstracts))
titles <- stringr::str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
titles <- str_replace_all(titles, "\\s+", " ")
table(is.na(titles))
journals <- stringr::str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journals <- str_remove_all(journals, "</?[[:alnum:]]+>")
journals <- str_replace_all(journals, "\\s+", " ")
table(is.na(journals))
dates <- stringr::str_extract(pub_char_list, "<PubDate>(\\n|.)+</PubDate>")
dates <- str_remove_all(dates, "</?[[:alnum:]]+>")
dates <- str_replace_all(dates, "\\s+", " ")
table(is.na(dates))
database <- data.frame(
PubMedId = ids,
Title = titles,
Journals = journals,
Date = dates,
Abstract = abstracts
)
knitr::kable(database)
dim(database)
df <- read.csv("pubmed.csv")
tokens <- df %>% select(abstract) %>% unnest_tokens(word, abstract) %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
library(wordcloud)
wordcloud(tokens$word, tokens$word_frequency)
tokens <- df %>% select(abstract) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
wordcloud(tokens$word, tokens$word_frequency)
tokens <- database %>% select(Abstract) %>% unnest_tokens(bigram, Abstract, token='ngrams', n=2) %>% group_by(bigram) %>% summarise(bigram_frequency = n()) %>% separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep=" ", fill="right") %>% anti_join(stop_words, by=c("word1"="word")) %>% anti_join(stop_words, by=c("word2"="word")) %>% arrange(across(bigram_frequency, desc))
tokens %>% head(10) %>% ggplot(aes(x=reorder(bigram, bigram_frequency), y=bigram_frequency)) + geom_bar(stat='identity') + coord_flip()
database %>% select(Abstract, Title) %>% unnest_tokens(word, Abstract) %>% anti_join(stop_words, by="word") %>% count(word, Title) %>% bind_tf_idf(word, Title, n) %>% arrange(desc(tf_idf))
View(df)
tokens <- df %>% select(abstract) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
wordcloud(tokens$word, tokens$word_frequency)
tokens <- df %>% select(abstract) %>% group_by(term) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
df <- df %>% group_by(term)
df <- df %>% group_by(term)
View(df)
tokens <- df %>% select(abstract) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(term, word) %>% summarize(word_frequency = n(), term=term) %>% arrange(across(word_frequency, desc)) %>% head(20)
View(tokens)
df <- read.csv("pubmed.csv")
tokens <- df %>% select(abstract) %>% unnest_tokens(word, abstract) %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
library(wordcloud)
wordcloud(tokens$word, tokens$word_frequency)
tokens <- df %>% select(abstract) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
wordcloud(tokens$word, tokens$word_frequency)
tokens <- df %>% select(abstract) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(term, word) %>% summarize(word_frequency = n(), term=term) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens <- df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(term, word) %>% summarize(word_frequency = n(), term=term) %>% arrange(across(word_frequency, desc)) %>% head(20)
View(tokens)
df %>% unnest_tokens(word, abstract)
tokens <- df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(term, word) %>% summarize(word_frequency = n(), term=term) %>% arrange(across(word_frequency, desc))
View(tokens)
tokens <- df %>% group_by(term) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n(), term=term) %>% arrange(across(word_frequency, desc))
View(tokens)
tokens <- df %>% group_by(term) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word")
View(tokens)
tokens <- df %>% group_by(term) %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n(), term=term) %>% distinct() %>% arrange(across(word_frequency, desc))
View(tokens)
tokens <- df %>% filter(term=="covid") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
View(tokens)
df %>% select(term) %>% distinct()
tokens <- df %>% filter(term=="covid") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
wordcloud(tokens$word, tokens$word_frequency)
tokens <- df %>% filter(term=="meningitis") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
wordcloud(tokens$word, tokens$word_frequency)
tokens <- df %>% filter(term=="covid") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
tokens <- df %>% filter(term=="meningitis") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
tokens <- df %>% filter(term=="prostate cancer") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
tokens <- df %>% filter(term=="cystic fibrosis") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
tokens <- df %>% filter(term=="preeclampsia") %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(5)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
tokens <- database %>% select(Abstract) %>% unnest_tokens(bigram, Abstract, token='ngrams', n=2) %>% group_by(bigram) %>% summarise(bigram_frequency = n()) %>% separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep=" ", fill="right") %>% anti_join(stop_words, by=c("word1"="word")) %>% anti_join(stop_words, by=c("word2"="word")) %>% arrange(across(bigram_frequency, desc))
tokens %>% head(10) %>% ggplot(aes(x=reorder(bigram, bigram_frequency), y=bigram_frequency)) + geom_bar(stat='identity') + coord_flip()
tokens <- df %>% select(abstract) %>% unnest_tokens(bigram, abstract, token='ngrams', n=2) %>% group_by(bigram) %>% summarise(bigram_frequency = n()) %>% separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep=" ", fill="right") %>% anti_join(stop_words, by=c("word1"="word")) %>% anti_join(stop_words, by=c("word2"="word")) %>% arrange(across(bigram_frequency, desc))
tokens %>% head(10) %>% ggplot(aes(x=reorder(bigram, bigram_frequency), y=bigram_frequency)) + geom_bar(stat='identity') + coord_flip()
df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% bind_tf_idf(word, term, n) %>% group_by(term) %>% arrange(desc(tf_idf)) %>% summarise(top_n(5))
df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% bind_tf_idf(word, term, n) %>% group_by(term) %>% arrange(desc(tf_idf))
df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% bind_tf_idf(word, term, n) %>% group_by(term) %>% arrange(desc(tf_idf)) %>% summarise(top_n(tf_idf, 5))
df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% bind_tf_idf(word, term, n) %>% group_by(term) %>% arrange(desc(tf_idf), .by_group = TRUE) %>% top_n(5)
df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% group_by(term) %>% arrange(desc(n), .by_group = TRUE) %>% top_n(5)
df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% bind_tf_idf(word, term, n) %>% group_by(term) %>% arrange(desc(tf_idf), .by_group = TRUE) %>% top_n(5)
df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% group_by(term) %>% arrange(desc(n), .by_group = TRUE) %>% top_n(5)
view(df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% group_by(term) %>% arrange(desc(n), .by_group = TRUE) %>% top_n(5))
tibble <- df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% group_by(term) %>% arrange(desc(n), .by_group = TRUE) %>% top_n(5)
view(tibble)
tibble <- df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% group_by(term) %>% arrange(desc(n), .by_group = TRUE) %>% top_n(5)
View(tibble)
tibble <- df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% group_by(term) %>% arrange(desc(n), .by_group = TRUE) %>% top_n(5)
print.data.frame(tibble)
tibble <- df %>% unnest_tokens(word, abstract) %>% anti_join(stop_words, by="word") %>% count(word, term) %>% bind_tf_idf(word, term, n) %>% group_by(term) %>% arrange(desc(tf_idf), .by_group = TRUE) %>% top_n(5)
print.data.frame(tibble)
