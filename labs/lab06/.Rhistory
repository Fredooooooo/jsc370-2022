knitr::opts_chunk$set(include  = TRUE)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2 vaccine")
knitr::opts_chunk$set(include  = TRUE)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2")
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
knitr::opts_chunk$set(include  = TRUE)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2 vaccine")
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2-vaccine")
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
View(ids)
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2-vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
View(ids)
View(ids)
View(ids)
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2-vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids[1]
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids$node[1]
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids$node
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
counts
# stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
stringr::str_extract(counts, "<Count>[0-9,]+<Count>")
stringr::str_extract(counts, "/<Count/>[0-9,]+/<Count/>")
stringr::str_extract(counts, "\<Count\>[0-9,]+\<Count\>")
counts
#stringr::str_extract(counts, "\<Count\>[0-9,]+\<Count\>")
stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
knitr::opts_chunk$set(include  = TRUE)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2-vaccine")
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
counts <- stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
counts <- stringr::str_remove(counts, "<Count>|</Count>")
counts
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
counts <- stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
counts <- stringr::str_remove_all(counts, "<Count>|</Count>")
counts
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = paste(ids, collapse=","),
retmax = 250,
rettype = "abstract"
)
)
# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = paste(ids, collapse=","),
retmax = 250,
rettype = "abstract"
)
)
# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
pub_char_list
knitr::opts_chunk$set(include  = TRUE)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2-vaccine")
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
counts <- stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
counts <- stringr::str_remove_all(counts, "<Count>|</Count>")
counts
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = paste(ids, collapse=","),
retmax = 250,
rettype = "abstract"
)
)
# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
pub_char_list
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
ids <- ids[:250]
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
knitr::opts_chunk$set(include  = TRUE)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2-vaccine")
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
counts <- stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
counts <- stringr::str_remove_all(counts, "<Count>|</Count>")
counts
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
ids <- ids[250]
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
ids <- ids[,250]
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(
db = "pubmed",
term = "sars-cov-2 vaccine",
retmax = 20000
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
counts <- as.character(ids)
counts <- stringr::str_extract(counts, "<Count>[0-9,]+</Count>")
counts <- stringr::str_remove_all(counts, "<Count>|</Count>")
counts
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[1-9]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
ids <- ids[1:250]
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = paste(ids, collapse=","),
retmax = 250,
rettype = "abstract"
)
)
# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
pub_char_list
pub_char_list
pub_char_list
pub_char_list[1]
pub_char_list[1]
pub_char_list[2]
dates <- stringr::str_extract(pub_char_list, "<PubDate>(\\n|.)+</PubDate>")
dates <- str_remove_all(dates, "</?[[:alnum:]]+>")
library(stringr)
library(stringr)
abstracts <- stringr::str_extract(pub_char_list, "<Abstract>(\\n|.)+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alpha:]]+>")
abstracts <- str_replace_all(abstracts, "\\s+", " ")
table(is.na(abstracts))
titles <- stringr::str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
titles <- str_replace_all(titles, "\\s+", " ")
table(is.na(titles))
dates <- stringr::str_extract(pub_char_list, "<PubDate>(\\n|.)+</PubDate>")
dates <- str_remove_all(dates, "</?[[:alnum:]]+>")
dates <- str_replace_all(dates, "\\s+", " ")
table(is.na(dates))
journals <- stringr::str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journals <- str_remove_all(journals, "</?[[:alnum:]]+>")
journals <- str_replace_all(journals, "\\s+", " ")
table(is.na(journals))
database <- data.frame(
PubMedId = ids,
Title = titles,
Journals = journals,
Date = dates,
Abstract = abstracts
)
knitr::kable(database)
dim(database)
library(stringr)
library(tidytext)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)
tokens <- database %>% select(Abstract) %>% unnest_tokens(word, Abstract) %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
library(wordcloud)
wordcloud(tokens$word, tokens$word_frequency)
tokens <- database %>% select(Abstract) %>% unnest_tokens(word, Abstract) %>% anti_join(stop_words, by="word") %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)
tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()
wordcloud(tokens$word, tokens$word_frequency)
