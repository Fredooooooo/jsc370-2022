---
title: "Lab 08 - Text Mining"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

-   Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text.
-   Use dplyr and ggplot2 to analyze text data

# Lab description

For this lab we will be working with the medical record transcriptions from <https://www.mtsamples.com/>. And is loaded and "fairly" cleaned at <https://github.com/JSC370/jsc370-2022/blob/main/data/medical_transcriptions/mtsamples.csv>.

This markdown document should be rendered using `github_document` document.

### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`. If you don't already have `tidytext` then you can install with

```{r, eval=FALSE}
install.packages("tidytext")
```

### read in Medical Transcriptions

Loading in reference transcription samples from <https://www.mtsamples.com/>

```{r, warning=FALSE, message=FALSE}
library(tidytext)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(wordcloud)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

------------------------------------------------------------------------

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different catagories do we have? Are these catagories related? overlapping? evenly distributed?

```{r}
mt_samples %>%
  count(medical_specialty, sort = TRUE) %>%
  ggplot(aes(medical_specialty, n)) + geom_col() + coord_flip()
```

We have 40 categories. These categories are not evenly distributed, and not overlapping.

------------------------------------------------------------------------

## Question 2

-   Tokenize the the words in the `transcription` column
-   Count the number of times each token appears
-   Visualize the top 20 most frequent words

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples %>% select(transcription) %>% unnest_tokens(word, transcription) %>% group_by(word) %>% summarize(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)

tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()

library(wordcloud)
wordcloud(tokens$word, tokens$word_frequency)
```

## We can see stop-words such as "the", "with", "and" appear the most in the text. This makes sense because such words are used frequently in any text, but they provides no insights about what this text is about.

## Question 3

-   Redo visualization but remove stopwords before
-   Bonus points if you remove numbers as well

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

```{r}
tokens <- mt_samples %>% select(transcription) %>% unnest_tokens(word, transcription) %>% anti_join(stop_words, by="word") %>% subset(!grepl("^\\d+$", word)) %>% group_by(word) %>% summarise(word_frequency = n()) %>% arrange(across(word_frequency, desc)) %>% head(20)

tokens %>% ggplot(aes(x=reorder(word, word_frequency), y=word_frequency)) + geom_bar(stat='identity') + coord_flip()

wordcloud(tokens$word, tokens$word_frequency)
```

We can see that the text is about Patient, history, disease, skins and etc. This gives us a better idea of the text after removing stop words.

------------------------------------------------------------------------

# Question 4

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams?

```{r}
tokens <- mt_samples %>% select(transcription) %>% unnest_tokens(bigram, transcription, token='ngrams', n=2) %>% group_by(bigram) %>% summarise(bigram_frequency = n()) %>% 
  separate(bigram, c("word1", "word2"), extra="drop", remove=F, sep=" ", fill="right") %>% 
  anti_join(stop_words, by=c("word1"="word")) %>% anti_join(stop_words, by=c("word2"="word")) %>%   subset(!grepl("\\d+", bigram)) %>% 
  arrange(across(bigram_frequency, desc))

tokens %>% head(20) %>% ggplot(aes(bigram, bigram_frequency)) + geom_bar(stat='identity') + coord_flip()
```

```{r}
tokens <- mt_samples %>% select(transcription) %>% unnest_tokens(trigram, transcription, token='ngrams', n=3) %>% group_by(trigram) %>% summarise(trigram_frequency = n()) %>% 
  separate(trigram, c("word1", "word2", "word3"), extra="drop", remove=F, sep=" ", fill="right") %>% 
  anti_join(stop_words, by=c("word1"="word")) %>% anti_join(stop_words, by=c("word2"="word")) %>%
  anti_join(stop_words, by=c("word3"="word")) %>% subset(!grepl("\\d+", trigram)) %>% 
  arrange(across(trigram_frequency, desc))

tokens %>% head(20) %>% ggplot(aes(trigram, trigram_frequency)) + geom_bar(stat='identity') + coord_flip()
```

## We can see that there are lots of bi-grams having close frequencies larger than 700, while there is only one "past medical history" has the frequency larger than 700 in tri-grams. We can also see that "blood pressure" appears the most in bi-grams, but it doesn't appear in top 20 tri-grams.

# Question 5

Using the results you got from question 4. Pick a word and count the words that appears after and before it.

```{r}
tokens %>% subset(word2 == "blood") %>% group_by(word1) %>% summarise(word1_freq = n()) %>% arrange(across(word1_freq, desc)) %>% head(20) %>% ggplot(aes(word1, word1_freq)) + geom_bar(stat='identity') + coord_flip()

tokens %>% subset(word2 == "blood") %>% group_by(word3) %>% summarise(word3_freq = n()) %>% arrange(across(word3_freq, desc)) %>% head(20) %>% ggplot(aes(word3, word3_freq)) + geom_bar(stat='identity') + coord_flip()

```

------------------------------------------------------------------------

# Question 6

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r}
mt_samples %>% unnest_tokens(word, transcription) %>% anti_join(stop_words, by="word") %>% subset(!grepl("^\\d+$", word)) %>% group_by(medical_specialty) %>% count(word, sort=T) %>% top_n(1,n) %>% ggplot(aes(x=medical_specialty, y=n, fill=word)) + geom_bar(stat = "identity") + coord_flip()
```

# Question 7 - extra

Find your own insight in the data:

Ideas:

-   Interesting ngrams

-   See if certain words are used more in some specialties then others

    ```{r}
    mt_samples %>% select(transcription) %>% unnest_tokens(quadgram, transcription, token='ngrams', n=4) %>% count(quadgram, sort=T) %>% top_n(20,n) %>% ggplot(aes(quadgram, n)) + geom_bar(stat = "identity") + coord_flip()
    ```

    We can see that the top 3 quad-grams are "the patient is a", "prepped and draped in", and "to the operating room".

# Deliverables

1.  Questions 1-7 answered, pdf or html output uploaded to quercus
